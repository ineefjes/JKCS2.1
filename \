#import scipy as sp
import os
import subprocess
import numpy as np

def cost_function(arr):
    with open('parameter.txt', 'w') as file:
        file.write(' '.join(map(str, arr))) 
    process = subprocess.Popen(['bash', 'XTB3_runXTB.sh'])
    process.wait()
    with open('result', 'r') as file:
            number_str = file.readline().strip()
            return float(number_str)

with open('initial.txt', 'r') as file:
  line = file.readline()
  initial0 = [float(i) for i in line.split()]

#model = sp.optimize.minimize(test, x0=initial, method='L-BFGS-B',options={'ftol':1e-4,'gtol':1e-5,'eps':1e-4})
#print(model)

diff_step = 1e-5
hess_step = 10*diff_step
max_iter = 20
initial = len(initial0)*[1]


#JK optimizer
def numeric_gradient(parr,diff_step,og_cost):
    return np.array([cost_function(generate_step(initial0*parr,idx,diff_step))-og_cost for idx in range(len(parr))])

def generate_step(parr,idx,diff_step):
    new_parr = parr.copy()
    new_parr[idx] = new_parr[idx]+diff_step
    return np.array(new_parr)

def numeric_hessian(gradient_norm,hess_step,parr,diff_step,og_cost):
    cost_stepped = cost_function(initial0*(parr + diff_step * gradient_norm))
    cost_hess_stepped = cost_function(initial0*(parr + (hess_step + diff_step) * gradient_norm))
    cost_hess = cost_function(initial0*(parr + hess_step * gradient_norm))
    print([og_cost,cost_stepped,cost_hess,cost_hess_stepped])
    return ((cost_stepped - og_cost) * hess_step) / (cost_hess_stepped - cost_hess - cost_stepped + og_cost)


def lets_optimize(parr,diff_step,og_cost,hess_step):
    friction = 0.99
    print(f">>> Optimizing for input {initial0}", flush = True)
    parr = np.array(parr)
    print('Original:',og_cost, flush = True)
    for iter in range(max_iter):
        print(f'>>> Iteration {iter}', flush = True)
        gradient = numeric_gradient(parr,diff_step,og_cost)
        gradient_norm = gradient/np.linalg.norm(gradient)
        print('Gradient:',gradient_norm, flush = True)
        move = numeric_hessian(gradient_norm,hess_step,parr,diff_step,og_cost)
        print('Move :', move, flush = True)
        if np.abs(move) < hess_step:
            return (parr,og_cost)
        next_test = 0
        while next_test == 0:
          parr_test = parr-friction*move*gradient_norm
          og_cost_test = cost_function(parr_test*initial0)
          print('Test :', og_cost_test, flush = True)
          if og_cost_test < og_cost:
            next_test = 1
          else:
            friction = 0.9*friction
        par = parr_test
        og_cost = og_cost_test
          
        print(parr*initial0, og_cost, flush = True)
    return (parr,og_cost)


og_cost = cost_function(initial0)
print(lets_optimize(initial,diff_step,og_cost,hess_step))


